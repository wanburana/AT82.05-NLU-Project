{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f068c1009b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import random\n",
    "import copy\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(file_path, do_lower_case=True):\n",
    "    examples = []\n",
    "    \n",
    "    with open('{}/seq.in'.format(file_path),'r',encoding=\"utf-8\") as f_text, open('{}/label'.format(file_path),'r',encoding=\"utf-8\") as f_label:\n",
    "        for text, label in zip(f_text, f_label):\n",
    "            \n",
    "            e = Inputexample(text.strip(),label=label.strip())\n",
    "            examples.append(e)\n",
    "            \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inputexample(object):\n",
    "    def __init__(self,text_a,label = None):\n",
    "        self.text = text_a\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "embed_dim = 768\n",
    "batch_size = 32\n",
    "lr= 0.001  # you can adjust \n",
    "temp = 0.3  # you can adjust \n",
    "lamda = 0.01  # you can adjust  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_5s = f'./BANKING77/train_5/'\n",
    "path_test = f'./BANKING77/test/'\n",
    "path_valid = f'./BANKING77/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where do i pay with my debit or credit card?\n",
      "card_acceptance\n"
     ]
    }
   ],
   "source": [
    "train_samples = load_examples(path_5s)\n",
    "print((train_samples[55].text))\n",
    "print((train_samples[55].label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self,labels,word_vector,batch_size,repeated_label:bool=False):\n",
    "        self.labels = labels\n",
    "        self.word_vector = word_vector\n",
    "        self.batch_size = batch_size \n",
    "        self.count = 0 \n",
    "        self.repeated_label = repeated_label\n",
    "\n",
    "        if self.repeated_label == True:\n",
    "            self.used_idx = []\n",
    "          \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        data = self.word_vector[idx]\n",
    "        \n",
    "        sample = {\"Class\": label,\"Text\": data}\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from InferSent.models import InferSent\n",
    "V = 2\n",
    "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://downgit.github.io/#/home?url=https:%2F%2Fgithub.com%2Fjianguoz%2FFew-Shot-Intent-Detection%2Ftree%2Fmain%2FDatasets%2FHWU64\n",
    "\n",
    "# downloading training samples \n",
    "train_samples = load_examples(path_5s)\n",
    "# write code here : for downloading validation samples\n",
    "valid_samples = load_examples(path_valid)\n",
    "#write code here : for downloading test samples\n",
    "test_samples = load_examples(path_test)\n",
    "\n",
    "# preprocess for training \n",
    "train_data = []\n",
    "train_label = []\n",
    "for i in range(len(train_samples)):\n",
    "    train_data.append(train_samples[i].text)\n",
    "    train_label.append(train_samples[i].label)\n",
    "    \n",
    "# write code here :preprocess validation samples\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "for i in range(len(valid_samples)):\n",
    "    valid_data.append(valid_samples[i].text)\n",
    "    valid_labels.append(valid_samples[i].label)\n",
    "# write code here : preprocess test samples\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for i in range(len(valid_samples)):\n",
    "    test_data.append(test_samples[i].text)\n",
    "    test_labels.append(test_samples[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 500000.0\n"
     ]
    }
   ],
   "source": [
    "infersent.build_vocab_k_words(5e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for training \n",
    "train_data = CustomTextDataset(train_label,train_data,batch_size=batch_size,repeated_label=True)\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# write code here : dataloader for validation\n",
    "valid_data = CustomTextDataset(valid_labels,valid_data,batch_size=batch_size,repeated_label=True)\n",
    "valid_loader = DataLoader(valid_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# write code here : dataloader for test\n",
    "test_data = CustomTextDataset(test_labels,test_data,batch_size=batch_size,repeated_label=True)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# got the number of unique classes from dataset\n",
    "num_class = len(np.unique(np.array(train_label)))\n",
    "\n",
    "# get text label of uniqure classes\n",
    "unique_label = np.unique(np.array(train_label))\n",
    "\n",
    "# map text label to index classes\n",
    "label_maps = {unique_label[i]: i for i in range(len(unique_label))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferSentClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InferSentClassification, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 77)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.fc1(input)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InferSentClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # freeze encoder layer\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'InferSent' in name:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasit/Documents/NLP-project/InferSent/models.py:207: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sentences = np.array(sentences)[idx_sort]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 56.613510608673096 training accuracy 0.015584414824843407\n",
      "Validation acc 0.02207792177796364\n",
      "Training loss 55.84168481826782 training accuracy 0.04935064911842346\n",
      "Training loss 54.990524768829346 training accuracy 0.08311688154935837\n",
      "Training loss 53.73274087905884 training accuracy 0.12467531859874725\n",
      "Training loss 52.86895489692688 training accuracy 0.15844155848026276\n",
      "Training loss 51.19095849990845 training accuracy 0.20519480109214783\n",
      "Validation acc 0.20064935088157654\n",
      "Training loss 50.0514030456543 training accuracy 0.26753246784210205\n",
      "Training loss 47.68717050552368 training accuracy 0.26493504643440247\n",
      "Training loss 46.33394527435303 training accuracy 0.3194805085659027\n",
      "Training loss 44.95954966545105 training accuracy 0.350649356842041\n",
      "Training loss 42.83592891693115 training accuracy 0.41038960218429565\n",
      "Validation acc 0.2889610230922699\n",
      "Training loss 41.913785219192505 training accuracy 0.4311688244342804\n",
      "Training loss 40.14120125770569 training accuracy 0.42077919840812683\n",
      "Training loss 37.73431968688965 training accuracy 0.5116882920265198\n",
      "Training loss 36.13520431518555 training accuracy 0.5038961172103882\n",
      "Training loss 36.063111543655396 training accuracy 0.5038961172103882\n",
      "Validation acc 0.37142857909202576\n",
      "Training loss 33.323771715164185 training accuracy 0.5428571105003357\n",
      "Training loss 31.037512063980103 training accuracy 0.5636363625526428\n",
      "Training loss 31.590438961982727 training accuracy 0.5636363625526428\n",
      "Training loss 28.48981261253357 training accuracy 0.5714285373687744\n",
      "Training loss 26.754602193832397 training accuracy 0.633766233921051\n",
      "Validation acc 0.41688311100006104\n",
      "Training loss 25.88978898525238 training accuracy 0.6051948070526123\n",
      "Training loss 25.325765371322632 training accuracy 0.6909090876579285\n",
      "Training loss 25.681841254234314 training accuracy 0.701298713684082\n",
      "Training loss 22.968436002731323 training accuracy 0.6805194616317749\n",
      "Training loss 21.28365921974182 training accuracy 0.71688312292099\n",
      "Validation acc 0.4701298475265503\n",
      "Training loss 21.309566974639893 training accuracy 0.6883116960525513\n",
      "Training loss 20.544538855552673 training accuracy 0.7272726893424988\n",
      "Training loss 18.26493787765503 training accuracy 0.7350649237632751\n",
      "Training loss 18.04308044910431 training accuracy 0.7038961052894592\n",
      "Training loss 16.322813510894775 training accuracy 0.7558441162109375\n",
      "Validation acc 0.5136363506317139\n",
      "Training loss 16.451117038726807 training accuracy 0.7558441162109375\n",
      "Training loss 15.617669939994812 training accuracy 0.8129869699478149\n",
      "Training loss 14.470698552206159 training accuracy 0.7610389590263367\n",
      "Training loss 15.881898701190948 training accuracy 0.8077921867370605\n",
      "Training loss 13.383545581251383 training accuracy 0.8103895783424377\n",
      "Validation acc 0.5220779180526733\n",
      "Training loss 12.935884654521942 training accuracy 0.8623376488685608\n",
      "Training loss 13.233816146850586 training accuracy 0.8519480228424072\n",
      "Training loss 12.00778080523014 training accuracy 0.8103895783424377\n",
      "Training loss 12.391594231128693 training accuracy 0.8623376488685608\n",
      "Training loss 11.095792979001999 training accuracy 0.864935040473938\n",
      "Validation acc 0.534415602684021\n",
      "Training loss 11.25297623872757 training accuracy 0.84935063123703\n",
      "Training loss 10.201915562152863 training accuracy 0.8961038589477539\n",
      "Training loss 10.390152931213379 training accuracy 0.8883116841316223\n",
      "Training loss 9.592450261116028 training accuracy 0.8727272748947144\n",
      "Training loss 9.126937061548233 training accuracy 0.916883111000061\n",
      "Validation acc 0.5493506193161011\n",
      "Training loss 8.364653393626213 training accuracy 0.9064934849739075\n",
      "Training loss 9.43138587474823 training accuracy 0.880519449710846\n",
      "Training loss 8.5687837600708 training accuracy 0.8909090757369995\n",
      "Training loss 7.953999817371368 training accuracy 0.9194805026054382\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model.cuda()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sentences = batch[\"Text\"]\n",
    "        sentence_vectors = torch.tensor(infersent.encode(sentences, bsize=128, tokenize=True, verbose=False))\n",
    "        labels = torch.tensor([label_maps[stringtoId] for stringtoId in (batch['Class'])])\n",
    "\n",
    "        sentence_vectors = sentence_vectors.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(sentence_vectors)\n",
    "        loss = criterion(outputs, labels)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_correct += torch.sum(predicted == labels.data)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    print(f'Training loss {train_loss} training accuracy {train_acc}')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        eval_correct = 0.0\n",
    "        best_val_acc = torch.inf\n",
    "        for batch in valid_loader:\n",
    "            sentences = batch[\"Text\"]\n",
    "            sentence_vectors = torch.tensor(infersent.encode(sentences, bsize=128, tokenize=True, verbose=False))\n",
    "            labels = torch.tensor([label_maps[stringtoId] for stringtoId in (batch['Class'])])\n",
    "            \n",
    "            sentence_vectors = sentence_vectors.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(sentence_vectors)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            eval_correct += torch.sum(predicted == labels.data)\n",
    "        eval_acc = eval_correct / len(valid_loader.dataset)\n",
    "        print(f'Validation acc {eval_acc}')\n",
    "        if eval_acc < best_val_acc:\n",
    "            best_val_acc = eval_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), \"infersent_calassification_10shot_new.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5494, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_acc"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ea266fb49e16d654e13fe0042c38ca5c2f386a61cf584105053315de3903c84"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
